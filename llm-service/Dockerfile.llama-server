FROM python:3.9

WORKDIR /app

# llama-cpp-pythonのインストール (プリビルド済みホイールを使用)
# GPUサポートが必要な場合は、CUDAベースイメージを使用し、CMAKE_ARGSを設定してください
RUN pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu

# llama-cpp-python[server]の依存関係を明示的にインストール
RUN pip install fastapi uvicorn sse_starlette starlette_context pydantic_settings

COPY . .

CMD python -m llama_cpp.server --model ${MODEL_PATH} --host 0.0.0.0 --port 8000 --n_gpu_layers ${N_GPU_LAYERS} --n_ctx ${N_CTX}